---
title: 'v1/chat/completions/'
weight: 1000
---

مدل‌های تولید متن قابل دسترس از طریق Gilas API توانایی درک زبان طبیعی و کدنویسی را دارند. این مدل‌ها به ورودی‌های خود با خروجی‌های متنی که بسیار شبیه به جواب‌های انسانی هستند پاسخ می‌دهند. ورودی‌های این مدل‌ها همچنین به عنوان "prompt" شناخته می‌شوند. طراحی یک پرامپت اساساً چگونگی "برنامه‌ریزی" یک مدل زبانی بزرگ است.

شما می‌توانید با استفاده از این مدل‌ها برنامه‌هایی بسازید که توانایی انجام کارهای زیر را دارند:
-	پیش‌نویس اسناد
-	نوشتن کد کامپیوتری
-	پاسخ به سوالات درباره پایگاه داده‌های متنی
-	تحلیل متون
-	ارائه رابط زبان طبیعی 
-	تعلیم علوم مختلف به زبان انسانی
-	ترجمه زبان‌های مختلف
-	شبیه‌سازی شخصیت‌ها برای بازی‌های کامپیوتری

برای استفاده از این مدل‌ها کافی است که یک درخواست شامل ورودی‌ها و کلید API خود را به اندپوینت مناسب ارسال کنید.

## Chat Completions API

مدل‌های تولید متن لیستی از پیام‌ها را به عنوان ورودی دریافت کرده و یک پیام تولید شده توسط مدل را به عنوان خروجی ارائه می‌دهند. اگرچه اندپویت چت برای ایجاد گفتگوهای چند مرحله‌ای طراحی شده است، اما از این اندپوینت  برای انجام وظایف  تک‌ مرحله‌ای بدون هیچ گفتگویی نیز می‌توان استفاده کرد.

## ساخت حساب کاربری

ابتدا، یک  [حساب کاربری جدید](https://dashboard.gilas.io) ایجاد کنید یا اگر صاحب حساب کاربری هستید [وارد پنل کاربری](https://dashboard.gilas.io) خود شوید. سپس، به صفحه [کلید API](https://dashboard.gilas.io/apiKey)  بروید و با کلیک روی دکمه “ساخت کلید API” یک کلید جدید برای دسترسی به Gilas API بسازید. مطمئن شوید که این کلید را در جای امنی ذخیره کرده و آن را با کسی به اشتراک نگذارید.

## ارسال درخواست
نمونه‌ای از فواخوانی  APIی چت را در زیر مشاهده کنید:


{{< tabs "quick start - code" >}}
{{< tab "curl" >}}

```shell
curl https://api.gilas.io/v1/chat/completions  \
-H "Content-Type: application/json" \
-H "Authorization: Bearer $GILAS_API_KEY" \
-d '{
    "model": "gpt-4o-mini",
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant."
      },
      {
        "role": "user",
        "content": "چه تیمی برنده رقابت‌های سری جهانی ۲۰۲۰ شد؟"
      },
      {
        "role": "assistant",
        "content": "تیم لس آنجلس دوجرز برنده رقابت‌های سری جهانی ۲۰۲۰ شد."
      },
      {
        "role": "user",
        "content": "بازی در کجا برگذار شده بود؟"
      }
    ]
  }'
```

{{< /tab >}}
{{< tab "Go" >}}


```go
package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io/ioutil"
	"net/http"
	"os"
)

func main() {
	url := "https://api.gilas.io/v1/chat/completions"
	payload := map[string]interface{}{
		"model": "gpt-4o-mini",
		"messages": []map[string]string{
			{
				"role":    "system",
				"content": "You are a helpful assistant.",
			},
			{
        "role": "user",
        "content": "چه تیمی برنده رقابت‌های سری جهانی ۲۰۲۰ شد؟"
      },
      {
        "role": "assistant",
        "content": "تیم لس آنجلس دوجرز برنده رقابت‌های سری جهانی ۲۰۲۰ شد."
      },
      {
        "role": "user",
        "content": "بازی در کجا برگذار شده بود؟"
      },
		},
	}

	jsonPayload, _ := json.Marshal(payload)
	req, _ := http.NewRequest("POST", url, bytes.NewBuffer(jsonPayload))
	req.Header.Set("Content-Type", "application/json")
	req.Header.Set("Authorization", "Bearer "+os.Getenv("GILAS_API_KEY"))

	client := &http.Client{}
	resp, _ := client.Do(req)
	defer resp.Body.Close()

	body, _ := ioutil.ReadAll(resp.Body)
	fmt.Println(string(body))
}
```

{{< /tab >}}

{{< tab "Node.js" >}}

```js
const axios = require('axios');

async function makeRequest() {
    const url = 'https://api.gilas.io/v1/chat/completions';
    const payload = {
        model: 'gpt-4o-mini',
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
              "role": "user",
              "content": "چه تیمی برنده رقابت‌های سری جهانی ۲۰۲۰ شد؟"
            },
            {
              "role": "assistant",
              "content": "تیم لس آنجلس دوجرز برنده رقابت‌های سری جهانی ۲۰۲۰ شد."
            },
            {
              "role": "user",
              "content": "بازی در کجا برگذار شده بود؟"
            }
        ]
    };

    try {
        const response = await axios.post(url, payload, {
            headers: {
                'Content-Type': 'application/json',
                'Authorization': `Bearer ${process.env.GILAS_API_KEY}`
            }
        });
        console.log(response.data);
    } catch (error) {
        console.error('Error making the request:', error);
    }
}

makeRequest();
```
{{< /tab >}}

{{< tab "Python" >}}

```python
import requests
import os

def make_request():
    url = 'https://api.gilas.io/v1/chat/completions'
    headers = {
        'Content-Type': 'application/json',
        'Authorization': f'Bearer {os.getenv("GILAS_API_KEY")}'
    }
    payload = {
        'model': 'gpt-4o-mini',
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
              "role": "user",
              "content": "چه تیمی برنده رقابت‌های سری جهانی ۲۰۲۰ شد؟"
            },
            {
              "role": "assistant",
              "content": "تیم لس آنجلس دوجرز برنده رقابت‌های سری جهانی ۲۰۲۰ شد."
            },
            {
              "role": "user",
              "content": "بازی در کجا برگذار شده بود؟"
            }
        ]
    }

    response = requests.post(url, json=payload, headers=headers)
    return response.json()

if __name__ == "__main__":
    result = make_request()
    print(result)
```
{{< /tab >}}
{{< /tabs >}}

ورودی اصلی در این فراخوان API پارامتر Messages است. Messages باید یک آرایه از شیء پیام باشند، جایی که هر شیء شامل role (با مقادیر "system"، "user" یا "assistant") و content است. چت ها می‌توانند به عنوان یک پیام کوتاه یا چندین نوبت پشت سر هم قرار بگیرند.

به طور معمول، یک گفتگو با یک پیام سیستمی (role=system) شروع می‌شود، سپس پیام‌های کاربر (role=user) و دستیار (role=assistant) به تناوب دنبال می‌شوند.
پیام سیستمی به تنظیم رفتار assistant کمک می‌کند. به عنوان مثال، می‌توانید شخصیت assistant را تغییر دهید یا دستورالعمل‌های خاصی را در مورد چگونگی رفتار آن در طول گفتگو ارائه دهید. با این حال توجه داشته باشید که پیام سیستمی اختیاری است.

از طریق پیام‌های کاربر٬ درخواست‌ها به assistant برای دریافت پاسخ ارسال می‌شوند. پیام‌های assistant معمولا از سمت مدل ساخته می‌شوند، اما شما نیز می‌توانید این نوع پیام‌ها را برای شبیه‌سازی رفتار سیستم و راهنمایی مدل برای تولید جواب‌هایی مشابه تولید کنید. 

زمانی که دستورالعمل‌ها یا درخواست‌های کاربر به پیام‌های قبلی ارجاع می‌دهند قرار دادن تاریخچه مکالمه در لیست Messages مهم است. در مثال بالا، سوال نهایی کاربر در مورد “?Where was is played“ تنها در کانتکست پیام‌های قبلی در مورد جام جهانی 2020 معنی دارد. از آنجا که مدل‌ها stateless هستند و درخواست‌های قبلی را ذخیره نمی‌کنند، تمام اطلاعات مربوطه باید به عنوان بخشی از تاریخچه گفتگو در هر درخواست فراهم شود. اگر طول Messages از لحاظ تعداد توکن از تعداد توکن مجازی که در کانتکست مدل می‌تواند قرار بگیرد بیشتر شود، باید پیام‌ها را به شکلی کوتاه کنید. برای اینکار می‌توانید پیام‌ها را ابتدا خلاصه کرده و سپس در لیست Messages نگه‌داری کنید. برای خلاصه‌سازی پیام‌ها می‌توانید یک درخواست جدا به مدل ارسال کنید و از آن بخواهید که متن (پیام) دریافت شده را خلاصه کرده و به شما برگرداند.

برای آگاهی از طول کانتکست هر مدل لطفا مستندات مربوط به [مدل‌های قابل دسترس](/models) را مطالعه کنید.

## Chat Completions response format

 نمونه از فرمت پاسخ Chat Completions API در زیر آمده است:

```shell
{
  "choices": [
    {
      "finish_reason": "stop",
      "index": 0,
      "message": {
        "content": "The 2020 World Series was played in Texas at Globe Life Field in Arlington.",
        "role": "assistant"
      },
      "logprobs": null
    }
  ],
  "created": 1677664795,
  "id": "chatcmpl-7QyqpwdfhqwajicIEznoc6Q47XAyW",
  "model": "gpt-4o-mini",
  "object": "chat.completion",
  "usage": {
    "completion_tokens": 17,
    "prompt_tokens": 57,
    "total_tokens": 74
  }
}
```

 پاسخ assistant در مسیر `choices[0].message.content` قرار دارد.


هر پاسخ شامل یک `finish_reason` خواهد بود. مقادیر ممکن برای `finish_reason` عبارت‌اند از:

- `stop`:  پیام کامل توسط `API` برگردانده شد است، یا پیام توسط یکی از توالی‌های `stop` ارائه‌شده از طریق پارامتر `stop` خاتمه یافته است.
  
- `length`: خروجی مدل  به دلیل پارامتر `max_tokens`  یا محدودیت تعداد توکن ناقص است.

- `function_call`: مدل تصمیم به فراخوانی یک تابع گرفته است. (برای آگاهی بیشتر از نحوه فراخوانی توابع برنامه توسط مدل٬ پست [فراخوانی توابع توسط مدل](/posts/how_to_call_functions_with_chat_models/) را مطالعه کنید.)

- `content_filter`: محتوا به دلیل مغایرت با قوانین OpenAI یا پلتفرم گیلاس توسط فیلترهای محتوا حذف شده است.

- `null`: پاسخ `API` هنوز در حال تولید یا ناقص است.

بسته به پارامترهای ورودی، پاسخ مدل ممکن است شامل اطلاعات مختلفی باشد.


## تولید خروجی با فرمت JSON

یک روش رایج برای استفاده از `Chat Completions` این است که مدل را به گونه‌ای پرامپت کنید که همیشه یک شیء `JSON` را برگرداند. این روش مخصوصا برای زمانی که قصد دارید از خروجی مدل در داخل برنامه‌های خود استفاده کنید بسیار کاربردی است. در نظر داشته باشید که ممکن از در بعضی حالات خروجی `JSON` تولید شده معتبر نباشد. برای همین همیشه خروجی را قبل از استفاده اعتبارسنجی کنید.

برای جلوگیری از این خطاها و بهبود عملکرد مدل، هنگام فراخوانی `gpt-4-turbo`، می‌توانید پارامتر `response_format` را به `{"type": "json_object"}` تنظیم کنید تا حالت `JSON` فعال شود. وقتی حالت `JSON` فعال است، مدل فقط به تولید رشته‌هایی محدود می‌شود که به شیء `JSON` معتبر تجزیه شوند.

نکات مهم:

- وقتی از حالت `JSON` استفاده می‌کنید، همیشه مدل را برای تولید خروجی `JSON` پرامپت کنید. برای این کار می‌توانید از پیام سیستمی برای درخواست صریح از مدل برای تولید خروجی با فرمت `JSON` استفاده کنید. 

- شیء `JSON` برگردانده شده ممکن است به دلیل بزرگ بودن طول خروجی و تمام شدن تعداد توکن‌هایی که می‌شود در خروجی تولید کرد ناقص بماند. برای مطمین شدن ازینکه خروجی تولید شده کامل است حتما ابتدا مقدار پارامتر `finish_reason` را بررسی کنید.

- حالت `JSON` تضمین نمی‌کند که خروجی با هر نوع `schema` خاصی مطابقت داشته باشد، فقط این اطمینان را می‌دهد که خروجی معتبر است و بدون خطا پارس می‌شود.


```bash
curl https://api.gilas.io/v1/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $GILAS_API_KEY" \
  -d '{
    "model": "gpt-4o-mini",
    "response_format": { "type": "json_object" },
    "messages": [
      {
        "role": "system",
        "content": "You are a helpful assistant designed to output JSON."
      },
      {
        "role": "user",
        "content": "چه تیمی برنده رقابت‌های سری جهانی ۲۰۲۰ شد؟"
      }
    ]
  }'
```

خروجی تولید شده به فرمت `JSON`:

```bash
"content": "{\"winner\": \"تیم لس آنجلس دوجرز\"}"`
```


## استریم کردن خروجی

به طور پیش فرض، هنگامی که شما از Gilas API درخواست تولید متن می کنید، کل متن تولید شده در یک رسپانس به کلاینت برگردانده می شود. اگر قصد تولید خروجی‌های طولانی را دارید٬ تولید متن می تواند چندین ثانیه طول بکشد.
برای دریافت سریعتر پاسخ ها٬ می توانید متن را در حالی که تولید می شود `stream` کنید. در این صورت پاسخ های
ناتمام را به صورت استریم از سمت سرور دریافت خواهید کرد.

برای استریم کردن متن ها، هنگام فراخوانی `v1/chat/completions` مقدار پارامتر `stream=true` را تنظیم کنید.
این درخواست یک شیء برمی گرداند که پاسخ را به عنوان [data-only server-sent events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events/Using_server-sent_events#event_stream_format) برگشت می دهد. 

توجه داشته باشید که استفاده از `stream=true` باعث می شود که مدیریت محتوای متن دشوارتر شود،
زیرا کار ارزیابی تکه های جزئی متن ممکن است دشوارتر باشند. مخصوصا زمانی که قصد تولید خروجی `JSON` را دارید توصیه می‌شود از حالت استریم استفاده نکنید.

یکی دیگر از معایب پاسخ های استریم این است که پاسخ دیگر شامل فیلد `usage` نیست که به شما بگوید چند توکن مصرف شده است. 
برای حل این مشکل٬ پس از دریافت و ترکیب همه پاسخ ها، می توانید تعداد توکن‌های مصرف شده را با استفاده از [tiktoken](/posts/how_to_count_tokens_with_tiktoken) محاسبه کنید.


خروجی تولید شده به صورت استریم شامل قطعه‌هایی (chunk) از جواب هستند:

```bash
{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"role":"assistant","content":""},"logprobs":null,"finish_reason":null}]}
{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{"content":"Hello"},"logprobs":null,"finish_reason":null}]}
....
{"id":"chatcmpl-123","object":"chat.completion.chunk","created":1694268190,"model":"gpt-4o-mini", "system_fingerprint": "fp_44709d6fcb", "choices":[{"index":0,"delta":{},"logprobs":null,"finish_reason":"stop"}]}
```

## مدیریت توکن‌ها

مدل‌های زبانی متن‌ها را در قطعاتی به نام `token`ها می‌خوانند و می‌نویسند. در زبان انگلیسی، یک `token` می‌تواند به کوتاهی یک حرف یا به درازای یک کلمه (مثلا `a` یا `apple`) باشد و در برخی زبان‌ها، `token`ها می‌توانند حتی کوتاه‌تر از یک حرف یا طولانی‌تر از یک کلمه باشند. 

برای مثال، رشته `"Gilas API is great!"` به شش `token` تقسیم می‌شود: `["Gil", "as", "API", " is", " great", "!"]`. 

تعداد کل `token`ها در یک فراخوانی API تاثیر می‌گذارد بر:
-	هزینه‌ی فراخوانی API٬ چرا که تولید و مصرف هر `token` هزینه دارد.
- مدت زمان فراخوانی API ، چرا که نوشتن `token`های بیشتر زمان بیشتری می‌برد.
-	عملکرد فراخوانی API، چرا که تعداد کل `token`ها باید کمتر از حداکثر طول کانتکست مدل باشد.

هزینه استفاده از APIهای تولید متن بر اساس تعداد توکن‌های تولید شده و مصرف شده محاسبه می‌شود. برای آگاهی بیشتر از هزینه‌ی استفاده از هر مدل مستندات مربوط به [هزینه‌ها](/pricing) را مطالعه کنید.

همچنین خواندن پست [شمردن تعداد توکن‌ها با ‌tiktoken](/posts/how_to_count_tokens_with_tiktoken) را برای درک بهتر توکن‌ها و نحوه شمردن و محاسبه هزینه توصیه می‌کنیم.


## فراخوانی توابع

زمانی که از رابط زبان طبیعی برای برنامه‌های خود استفاده می‌کنید٬ در اغلب موارد لازم می‌شود تا برای انجام درخواست کاربر تعدادی از توابع داخل برنامه را فراخوانی کنید.
مثلا فرض کنید کاربر از برنامه بخواهد که برای یکی از دوستانش ایمیلی ارسال کند و او را به صرف شام دعوت کند٬ سپس این قرار ملاقات را در تقویم او نیز ثبت کند.
برای اجرای این کارها باید تعدادی از توابع داخلی برنامه که برای انجام این فرایندها برنامه‌نویسی شده‌اند فراخوانی شوند.

مدلهای `gpt-3.5-turbbo` و `gpt-4-turbo` قابلیت تصمیم گیری برای فراخوانی توابع داخل برنامه را بر اساس درخواست یوزر دارند.

در یک فراخوانی `API`، می‌توانید توابعی را توصیف کنید و به مدل اجازه دهید تا به طور هوشمندانه تصمیم بگیرد که یک شیء `JSON` شامل آرگومان‌هایی برای فراخوانی یک یا چند تابع را خروجی دهد. `Chat Completions API`  تابعی را فراخوانی نمی‌کند؛ به جای آن، مدل `JSON`ای تولید می‌کند که می‌توانید از آن برای فراخوانی تابع در کد خود استفاده کنید.

مدل‌های جدید (`gpt-4o-mini` و `gpt-4-turbo`) آموزش دیده‌اند که هم تشخیص دهند چه زمانی باید تابعی فراخوانی شود (بسته به ورودی) و هم پاسخی به فرم `JSON` بدهند که برای فراخوانی توابع ضروری است.
این توانایی، خطرات بالقوه‌ای نیز دارد. ما توصیه می‌کنیم که قبل از انجام هر گونه عملیاتی از طرف کاربران (فرستادن ایمیل، ارسال پست در اینترنت، خرید و غیره)، ورودی های کاربر را اعتبارسنجی کنید.

### کاربردهای متداول

فراخوانی تابع به شما امکان می‌دهد داده‌های ساختاریافته قابل اعتمادتری از مدل دریافت کنید. به عنوان مثال، می‌توانید:

-	دستیارهایی ایجاد کنید که با فراخوانی `API`های خارجی به سؤالات پاسخ دهند.
		
- زبان طبیعی را به فراخوانی‌های `API` تبدیل کنید
		مثلاً "چه کسانی مشتریان اصلی من هستند؟" را به `get_customers(min_revenue: int, created_before: string, limit: int)` تبدیل کرده و `API` داخلی خود را فراخوانی کنید.
    
... و خیلی چیزهای دیگر!

توالی مراحل برای فراخوانی تابع به شرح زیر است:

- مدل را با پرسش کاربر و مجموعه‌ای از توابع تعریف شده در پارامتر `functions` فراخوانی کنید.
-	مدل ممکن است تصمیم بگیرد یک یا چند تابع را فراخوانی کند؛ در این صورت، محتوا یک شیء `JSON`  خواهد بود که مطابق با schema تعریف شده شما است (توجه: مدل ممکن است در تولید پارامترها اشتباه کند).
- رشته را در کد خود به `JSON` تبدیل کرده و اگر آرگومان‌هایی وجود دارد، تابع خود را با آن‌ها فراخوانی کنید.
- مدل را دوباره با اضافه کردن پاسخ تابع به عنوان پیام جدید فراخوانی کنید و اجازه دهید مدل نتایج را به کاربر خلاصه کند.


### رفتار فراخوانی تابع

مقدار پیش‌فرض برای متغیر `tool_choice` هنگام فراخوانی اندپویت `v1/chat/completions/` معادل  `auto` است. ازین طریق این اجازه را به مدل می‌دهیم تا تصمیم بگیرد آیا توابعی را فراخوانی کند و در صورت مثبت بودن پاسخ، کدام توابع را فراخوانی کند.

ما سه روش برای سفارشی‌سازی رفتار پیش‌فرض بسته به مورد استفاده شما ارائه می‌دهیم:

-	برای مجبور کردن مدل به فراخوانی یک یا چند تابع، می‌توانید `tool_choice: required` را تنظیم کنید. ازین طریق مدل باید همیشه تابع یا توابعی را که باید فراخوانی شوند انتخاب می‌کند.
-	برای مجبور کردن مدل به فراخوانی تنها یک تابع خاص، می‌توانید `tool_choice: {"type": "function", "function": {"name": "my_function"}}` را تنظیم کنید.
-	برای غیرفعال کردن فراخوانی تابع و مجبور کردن مدل به تولید تنها یک پیام متنی برای کاربر، می‌توانید `tool_choice: none` را تنظیم کنید.

برای مشاهده یک مثال کامل از نحوه‌ی فراخوانی توابع می‌توانید پست [فراخوانی توابع توسط مدل](/posts/how_to_call_functions_with_chat_models) را مطالعه کنید.

## اطلاعات تکمیلی
در نظر داشته باشید که Gilas APIs از لحاظ فنی و نحوه کارکرد و قابلیت‌ها کاملا شبیه API ارایه دهنده های دیگر مانند OpenAI یا Mistral هستند. به همین منظور پیشنهاد میکنیم که برای آگاهی از نحوه‌ی کارکرد API ها به مستندات زیر مراجعه کنید.

- [OpenAI API Reference](https://platform.openai.com/docs/api-reference/chat)
- [OpenAI Documentation](https://platform.openai.com/docs/guides/text-generation) 
- [Mistral API Reference](https://docs.mistral.ai/api/#tag/chat)
- [Mistral Documentation](https://docs.mistral.ai/capabilities/completion/) 
