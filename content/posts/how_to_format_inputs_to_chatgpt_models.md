
---
title: "نحوه فرمت اطلاعات برای فراخوانی مدل"
description: "این Notebook نشان می‌دهد چگونه می‌توان از توانایی‌های بصری GPT-4 برای درک محتوای یک ویدیو و تولید متن متناسب با آن و نهایتا تبدیل متن تولید شده به صدا استفاده کرد. GPT-4 به طور مستقیم ویدیوها را به عنوان ورودی قبول نمی‌کند، اما می‌توانیم از قابلیت vision و طول کانتکست 128K برای توصیف فریم‌های این راهنما شامل دو مرحله است:"
tags:
- openai
- gpt-4
- gpt-4-vision
- text-to-speech
- video-processing
- gilas.io
- blog
weight: 2001
og_image: "/posts/how_to_format_inputs_to_chatgpt_models/banner.png" 
---

{{< postcover src="/posts/how_to_format_inputs_to_chatgpt_models/banner.png" >}}


مدل‌های چت، یگ سری از پیام‌ها را به عنوان ورودی می‌پذیرند و یک پیام نوشته شده توسط AI را به عنوان خروجی برمی‌گردانند.
این راهنما با چند نمونه فراخوانی API فرمت چت را نشان می‌دهد.



{{< hint info >}}
**برای اجرای کدهای زیر ابتدا باید یک کلید API را از طریق پنل کاربری گیلاس تولید کنید.  برای این کار
ابتدا یک  [حساب کاربری جدید](https://dashboard.gilas.io) بسازید یا اگر صاحب حساب کاربری هستید [وارد پنل کاربری](https://dashboard.gilas.io) خود شوید. سپس، به صفحه [کلید API](https://dashboard.gilas.io/apiKey)  بروید و با کلیک روی دکمه “ساخت کلید API” یک کلید جدید برای دسترسی به Gilas API بسازید.**
{{< /hint >}} 

```python
# if needed, install and/or upgrade to the latest version of the OpenAI Python library
%pip install --upgrade openai
```

```python
# imports
from openai import OpenAI # for calling the OpenAI API
import os # for getting API token from env variable OPENAI_API_KEY

client = OpenAI(
    api_key=os.environ.get(("GILAS_API_KEY", "<کلید API خود را اینجا بسازید https://dashboard.gilas.io/apiKey>")), 
    base_url="https://api.gilas.io/v1/" # Gilas APIs
)
```


## مثالی از فراخوانی Chat Completions API

برای فراخوانی Chat Completions API پارامترهای زیر باید به مدل ارسال شوند:

- `model`: نام مدلی که می‌خواهید استفاده کنید (مانند `gpt-3.5-turbo`, `gpt-4-turbo`)
- `messages`: لیستی از شیء‌های پیام، جایی که هر شیء دو فیلد ضروری دارد:
    - `role`: نقش فرستنده پیام (`system`، `user`، `assistant` یا `tool`)
    - `content`: محتوای پیام (مثلاً نوشتن یک شعر زیبا)

پیام‌ها همچنین می‌توانند یک فیلد اختیاری `name` را شامل شوند، که به فرستنده نام می‌دهد. مثلاً `example-user`، `Hichkas`. نام‌ها نباید شامل فاصله باشند.

پارامترهای اختیاری:

- `frequency_penalty`: توکن‌ها را بر اساس تعداد تکرارشان جریمه می‌کند، در نتیجه تکرار توکن‌ها را کم می‌کند.
- `logit_bias`: احتمال توکن‌های مشخصی را با مقادیر bias تغییر می‌دهد.
- `logprobs`: اگر `true` باشد، log احتمالات توکن‌های خروجی را برمی‌گرداند.
- `top_logprobs`: تعداد توکن‌های بیشتر محتمل را برای بازگرداندن در هر موقعیت مشخص می‌کند.
- `max_tokens`: حداکثر تعداد توکن‌های تولید شده در پاسخ را تنظیم می‌کند.
- `n`: تعداد مشخصی از پاسخ را برای هر ورودی تولید می‌کند.
- `presence_penalty`: توکن‌های جدید را بر اساس حضورشان در متن جریمه می‌کند.
- `response_format`: فرمت خروجی را مشخص می‌کند، مثلاً حالت JSON.
- `seed`: با یک `seed` مشخص، نمونه‌گیری قطعی را تضمین می‌کند.
- `stop`: تا 4 دنباله را مشخص می‌کند که API باید در آن‌ها تولید توکن را متوقف کند.
- `stream`: دلتاهای پیام‌های جزئی را به محض تولید توکن جدید می‌فرستد.
- `temperature`: درجه نمونه‌برداری را بین 0 و 2 تنظیم می‌کند.
- `top_p`: از نمونه‌برداری هسته استفاده می‌کند؛ توکن‌های با top_p حجم احتمال را در نظر می‌گیرد.
- `tools`: فهرست توابعی که مدل ممکن است صدا بزند.
- `tool_choice`: تماس‌های تابع مدل را کنترل می‌کند (none/auto/function).
- `user`: شناسه یکتا برای نظارت بر کاربر نهایی و تشخیص سوء استفاده.

از ژانویه 2024، می‌توانید یک لیست از توابع را ارسال کنید که به GPT بگوید آیا می‌تواند یک JSON را برای ورودی به یک تابع تولید کند. برای اطلاعات بیشتر می‌توانید [این پست](/how_to_call_functions_with_chat_models) را مطالعه کنید.


معمولاً، یک گفتگو با پیام `system` آغاز می‌شود که به `assistant` می‌گوید چگونه رفتار کند، و پس از آن پیام‌های `user` و `assistant` به نوبت دنبال می‌شوند، اما شما مجبور به دنبال کردن این فرمت نیستید.


بیایید نگاهی به یک نمونه فراخوانی API چت بیندازیم تا ببینیم فرمت چت در عمل چگونه کار می‌کند.

```python
# Example OpenAI Python library request
MODEL = "gpt-3.5-turbo"
response = client.chat.completions.create(
    model=MODEL,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Knock knock."},
        {"role": "assistant", "content": "Who's there?"},
        {"role": "user", "content": "Orange."},
    ],
    temperature=0,
)

print(json.dumps(json.loads(response.model_dump_json()), indent=4))
```
```
{
    "id": "chatcmpl-8dee9DuEFcg2QILtT2a6EBXZnpirM",
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "logprobs": null,
            "message": {
                "content": "Orange who?",
                "role": "assistant",
                "function_call": null,
                "tool_calls": null
            }
        }
    ],
    "created": 1704461729,
    "model": "gpt-3.5-turbo-0613",
    "object": "chat.completion",
    "system_fingerprint": null,
    "usage": {
        "completion_tokens": 3,
        "prompt_tokens": 35,
        "total_tokens": 38
    }
}
```

همانطور که می‌بینید، شیء پاسخ دارای چند فیلد است:

- `id`: شناسه درخواست
- `choices`: لیستی از اشیاء تکمیل (فقط یکی، مگر اینکه شما n را بیشتر از 1 تنظیم کنید)
    - `finish_reason`: دلیلی که مدل تولید متن را متوقف کرده است (یا `stop`، یا `length` اگر حد `max_tokens` رسیده باشد)
    - `index`: ایندکس انتخاب در لیست.
    - `logprobs`: اطلاعات احتمال لاگ برای انتخاب.
    - `message`: شیء پیام تولید شده توسط مدل
        - `content`: محتوای پیام
        - `role`: نقش نویسنده این پیام.
        - `tool_calls`: فراخوانی‌های ابزار تولید شده توسط مدل، مانند فذاخوانی‌های تابع. اگر ابزار داده شده باشد
- `created`: زمان درخواست
- `model`: نام کامل مدل استفاده شده برای تولید پاسخ
- `object`: نوع شیء برگشتی (مثلاً chat.completion)
- `system_fingerprint`:  اثر انگشت، پیکربندی پشتیبانی که مدل با آن اجرا می‌شود را نشان می‌دهد.
- `usage`: تعداد توکن‌های استفاده شده برای تولید پاسخ‌ها، شامل پرامپت، تکمیل و کل

/////////

ب فقط پاسخ با:

حتی وظایف غیرمبتنی بر گفتگو نیز می‌توانند در فرمت چت قرار بگیرند، با قرار دادن دستورالعمل در اولین پیام کاربر.
برای مثال، برای درخواست از مدل که برنامه‌نویسی ناهمزمان را به سبک دزد دریایی Blackbeard توضیح دهد، می‌توانیم گفتگو را به شکل زیر ساختاربندی کنیم:

نکاتی برای دستورالعمل دادن به gpt-3.5-turbo-0301
بهترین شیوه‌ها برای دستورالعمل دادن به مدل‌ها ممکن است از نسخه مدل به نسخه مدل تغییر کند. توصیه‌هایی که دنبال می‌شود برای gpt-3.5-turbo-0301 اعمال می‌شود و ممکن است برای مدل‌های آینده صدق نکند.
پیام‌های سیستمی
پیام سیستمی می‌تواند برای آماده‌سازی دستیار با شخصیت‌ها یا رفتارهای مختلف استفاده شود.
توجه داشته باشید که gpt-3.5-turbo-0301 به طور کلی به اندازه gpt-4-0314 یا gpt-3.5-turbo-0613 به پیام سیستمی توجه نمی‌کند. بنابراین، برای gpt-3.5-turbo-0301، ما پیشنهاد می‌کنیم دستورالعمل‌های مهم را در پیام کاربر قرار دهید. برخی از توسعه‌دهندگان موفقیت یافته‌اند که با قرار دادن پیام سیستمی در انتهای گفتگو، توجه مدل را از دور شدن به عنوان گفتگوها طولانی‌تر می‌شوند، حفظ کنند.

استفاده از نمونه‌های چندگانه برای دستورالعمل دادن
در برخی موارد، نشان دادن مدل به آنچه می‌خواهید، به جای گفتن به مدل آنچه می‌خواهید، آسان‌تر است.
یک راه برای نشان دادن مدل آنچه می‌خواهید، با پیام‌های نمونه جعلی است.
برای مثال:

برای کمک به روشن کردن اینکه پیام‌های نمونه بخشی از یک گفتگوی واقعی نیستند و نباید توسط مدل به آنها ارجاع داده شود، می‌توانید سعی کنید فیلد name پیام‌های system را به example_user و example_assistant تنظیم کنید.
تبدیل نمونه چندگانه بالا، می‌توانیم بنویسیم:

هر تلاشی برای مهندسی گفتگوها در ابتدا موفق نخواهد شد.
اگر اولین تلاش‌های شما شکست خورد، نترسید از آزمایش با روش‌های مختلف برای آماده‌سازی یا شرایط‌سازی مدل.
به عنوان مثال، یک توسعه‌دهنده افزایش دقت را کشف کرد زمانی که یک پیام کاربری را که می‌گفت "تاکنون کار عالی بوده، این‌ها کاملاً عالی بوده‌اند" قرار داد تا مدل را به ارائه پاسخ‌های با کیفیت بالاتر شرطی کند.
برای ایده‌های بیشتر درباره افزایش قابلیت اطمینان مدل‌ها، بررسی راهنمای ما در مورد تکنیک‌های افزایش قابلیت اطمینان را در نظر بگیرید. برای مدل‌های غیر چت نوشته شده است، اما بسیاری از اصول آن هنوز اعمال می‌شوند.
شمارش توکن‌ها
وقتی درخواست خود را ارسال می‌کنید، API پیام‌ها را به یک دنباله از توکن‌ها تبدیل می‌کند.
تعداد توکن‌های استفاده شده تأثیر می‌گذارد بر:
هزینه درخواست
زمان لازم برای تولید پاسخ
وقتی پاسخ به دلیل رسیدن به حداکثر محدودیت توکن (4,096 برای gpt-3.5-turbo یا 8,192 برای gpt-4) قطع می‌شود
شما می‌توانید از تابع زیر برای شمارش تعداد توکن‌هایی که یک لیست از پیام‌ها استفاده خواهد کرد، استفاده کنید.
توجه داشته باشید که روش دقیق شمارش توکن‌ها از پیام‌ها ممکن است از مدلی به مدل دیگر تغییر کند. شمارش‌هایی که از تابع زیر به دست می‌آید را یک تضمین همیشگی در نظر نگیرید.
به ویژه، درخواست‌هایی که از ورودی توابع اختیاری استفاده می‌کنند، توکن‌های اضافی را بر فراز تخمین‌های محاسبه شده زیر مصرف می‌کنند.
برای خواندن بیشتر درباره شمارش توکن‌ها به How to count tokens with tiktoken مراجعه کنید.
